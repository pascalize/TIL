MLS


Glue: JDBC接続で、 オンプレのRDBMS にアクセス可能
Datasync: オンプレNAS と S3 間の大容量ファイル転送

Data Firehose: ストリーミング。 
	└─ フォーマット変換機能はJSON, Common Log Format のどっちかを前提
Data Analytics: 即時SQLアクセス。 上記の後に置くと、リアルタイム変換もどきができる


Parquet: Athena, Glue などの列指向クエリ基盤
SageMaker: 基本はParquet、 画像分類はRecordIOプロトコルバッファ形式？

⭐⭐Glue は、⭐pysparkベースのフルマネージドETLサービス⭐ （問23）
	└─ この2つはセット！！

Sparkは、大規模データセットを高速に処理するための分散処理フレームワークであり、
PySparkはそのSparkをPythonから操作するためのインターフェースを提供

SageMaker Processing
データの前処理、後処理、バッチ推論などを独立したコンテナ環境で実行できる機能
Scikit-learn や Spark、独自コンテナを利用可能

大量のデータ: Glue より SageMaker Preprocessing (?)


=========

Kinesis Firehouse : 数十秒のラグあり
Kinesis data stream: 逐次処理、早いがコスト高め
⭐S3をKinesis Data Firehoseの送信先にすることが出来ますが、
S3をKinesis Data Streamsの送信先にする事は出来ません。⭐

Firehouse：転送に特化ぎみ？、秒単位になるが安い
	└─ S3に転送する「直前」に、レコード変換を挟める。その際にlambdaでjson, csvを加工・拡張できる
	※ S3にputしてからではなく、S3の直前
	※ csv, json を扱う際にはlambda必須
data stream：加工もできる、早い

=========


ロジスティック回帰：正と負、どっちにずれているかまで分かる！
t-SNE（t-distributed Stochastic Neighbor Embedding）
	高次元データを2次元や3次元の低次元空間に変換して可視化するための手法
	次元圧縮


RecordIO protobuf: 画像分類 (SageMaker) など
parquet: 分析系など。 画像系はNG


AWS Glue FindMatches
完全一致していない／主キーがないデータに対しても、データセット内の重複レコードや一致するレコードを識別する。
